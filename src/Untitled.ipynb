{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T16:54:41.880807Z",
     "iopub.status.busy": "2023-03-10T16:54:41.880509Z",
     "iopub.status.idle": "2023-03-10T16:54:41.886580Z",
     "shell.execute_reply": "2023-03-10T16:54:41.885467Z",
     "shell.execute_reply.started": "2023-03-10T16:54:41.880782Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T16:54:42.124860Z",
     "iopub.status.busy": "2023-03-10T16:54:42.124471Z",
     "iopub.status.idle": "2023-03-10T16:54:42.142424Z",
     "shell.execute_reply": "2023-03-10T16:54:42.141251Z",
     "shell.execute_reply.started": "2023-03-10T16:54:42.124829Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "class TSdataset:\n",
    "\n",
    "    def __init__(self, path:str, source:str):\n",
    "        self.source = source\n",
    "        self._features = {}\n",
    "        self.MinMaxs = MinMaxScaler()\n",
    "        if source ==\"NAB\":\n",
    "            split_name = str(path).split('/')\n",
    "            self.ds_name = '/'.join(split_name[-2:])\n",
    "            self.df = pd.read_csv(path, parse_dates=[0], index_col= 0)\n",
    "            self.ts = np.array(self.df.value)\n",
    "            self._features['DS_name']  = self.ds_name\n",
    "            self._get_NAB_anomaly()\n",
    "\n",
    "\n",
    "        elif source == \"UCR\":\n",
    "            split_name = str(path).split('/')\n",
    "            self.ds_name = '/'.join(split_name[-2:])\n",
    "            split_name = str(split_name[-1]).split('.')[0]\n",
    "            name_aux = str(split_name).split('_')\n",
    "            self.ds_name = f\"{split_name[-1]}\"\n",
    "            self.ts = np.genfromtxt(path)\n",
    "            self.df = pd.DataFrame(self.ts, columns = ['value'])\n",
    "            self._features['DS_name'] = self.ds_name\n",
    "            anomaly = np.zeros(len(self.df), dtype = np.int)\n",
    "            anomaly[int(name_aux[5]):int(name_aux[6])] = 1\n",
    "            self.df['is_anomaly'] = anomaly\n",
    "\n",
    "\n",
    "        elif source == \"YAHOO\":\n",
    "            split_name = str(path).split('/')\n",
    "            self.ds_name = '/'.join(split_name[-2:])\n",
    "            self.df = pd.read_csv(path)\n",
    "            self.df.set_index('timestamp', inplace = True)\n",
    "            self.ts = np.array(self.df['value'])\n",
    "            self._features['DS_name'] = self.ds_name\n",
    "            \n",
    "        self.ts_scaled = self.MinMaxs.fit_transform(self.ts.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def _get_NAB_anomaly(self, path:str = None):\n",
    "        if path == None:\n",
    "            with urllib.request.urlopen(\"https://raw.githubusercontent.com/numenta/NAB/master/labels/combined_windows.json\") as url:\n",
    "                an = json.load(url)\n",
    "        else:\n",
    "            with open(path, \"r\") as jsonF:\n",
    "                an = json.load(jsonF)\n",
    "\n",
    "        aux = np.zeros(len(self.df), dtype = np.int)\n",
    "        for start, end in an[self.ds_name]:\n",
    "            aux[self.df.index.get_loc(pd.to_datetime(start)): self.df.index.get_loc(pd.to_datetime(end))] = 1\n",
    "        self.df['is_anomaly'] = aux\n",
    "\n",
    "\n",
    "\n",
    "    def _get_anomaly_window(self):\n",
    "        edges = np.diff(np.concatenate([[0],self.df['is_anomaly'],[0]])).nonzero()[0]\n",
    "        edges = edges.reshape((-1,2)) + np.array([0,-1])\n",
    "        if self.source == 'NAB':\n",
    "            return np.array(self.df.index)[edges]\n",
    "        else:\n",
    "            return edges\n",
    "\n",
    "\n",
    "    def plot(self, width:int = 25, height:int = 8):\n",
    "\n",
    "        my_alpha = 0.4\n",
    "        plt.figure(figsize=(width,height))\n",
    "        if self.source in ['YAHOO','UCR']:\n",
    "            real_anoms = self._get_anomaly_window()\n",
    "\n",
    "            extend_window = 2\n",
    "            for anom in real_anoms:\n",
    "                plt.axvspan(anom[0]-extend_window,anom[1]+extend_window, ymin=0.0, ymax=50, alpha=my_alpha, color='red')\n",
    "            plt.plot(self.df['value'], zorder=1)\n",
    "            plt.ylim((self.df['value'].values.min(),self.df['value'].values.max()));\n",
    "        else:\n",
    "\n",
    "            real_anoms = self._get_anomaly_window()\n",
    "            for anom in real_anoms:\n",
    "                plt.axvspan(anom[0],anom[1], ymin=0.0, ymax=50, alpha=my_alpha, color='red')\n",
    "            plt.plot(self.df['value'], zorder=1)\n",
    "            plt.ylim((self.df['value'].values.min(),self.df['value'].values.max()));\n",
    "        plt.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = TSdataset('../../metaFeaturesTS/data/UCR_Anomaly_FullData/071_UCR_Anomaly_DISTORTEDltstdbs30791AS_23000_52600_52800.txt', 'UCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.ts_scaled[:int(len(ds.ts)*0.4)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIME_STEPS = 115*4\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(ds.ts_scaled[:int(len(ds.ts)*0.4)], TIME_STEPS)\n",
    "print(\"Training input shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "        layers.Conv1D(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    workers=-1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train_pred = model.predict(x_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold = 0.001\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train[100])\n",
    "plt.plot(x_train_pred[100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_value = ds.ts_scaled\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(df_test_value)\n",
    "plt.show()\n",
    "\n",
    "# Create sequences from test values.\n",
    "x_test = create_sequences(ds.ts_scaled)\n",
    "print(\"Test input shape: \", x_test.shape)\n",
    "\n",
    "# Get test MAE loss.\n",
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Detect all the samples which are anomalies.\n",
    "anomalies = test_mae_loss > 0.02\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomalies = test_mae_loss > 0.03\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomalous_data_indices = []\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n",
    "        anomalous_data_indices.append(data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_subset = ds.df.iloc[anomalous_data_indices]\n",
    "fig, ax = plt.subplots()\n",
    "ds.df.plot(legend=False, ax=ax)\n",
    "df_subset.plot(legend=False, ax=ax, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.plot(legend=False, ax=ax, color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
